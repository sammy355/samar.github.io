---
title: "Improved Off-policy Reinforcement Learning in Biological Sequence Design"
date: 2026-02-16
permalink: /posts/2026/02/RL_seq/
tags:
  - deep-learning
  - Reinforcement-Learning
  - Biological-Sequence-Design
  - Proxy-training
  - ùúπ-Conservative-search
  - GFlowNets
math: true
---

## Abstract

Designing biological sequences such as DNA, RNA, and proteins is a central challenge in biotechnology due to the Huge search space and the high cost of estimation. Recent reinforcement learning (RL) approaches rely on proxy models to estimate quality, but these models often fail when exploring regions with unknown data. This paper introduces Œ¥-Conservative Search (Œ¥-CS), a novel off-policy reinforcement learning strategy that balances exploration and reliability in biological sequence design. The method focus on high-scoring known sequences by randomly masking tokens and then denoising them using a generative policy, ensuring that new candidates remain close to reliable regions of the search space. The degree of conservativeness is adaptively adjusted based on the proxy model‚Äôs uncertainty. Experiments across DNA, RNA, and protein design tasks demonstrate that Œ¥-CS consistently discovers higher quality sequences than existing methods. This makes Œ¥-CS a scalable and reliable framework for real-world biological design.

## Table of Contents

- [Introduction and Motivation](#introduction-and-motivation)  
- [Reinforcement Learning for Sequence Design](#reinforcement-learning-for-sequence-design)
- [The Core Problem: Proxy Misspecification](#the-core-problem-proxy-misspecification)  
- [Key Idea: Delta Conservative Search](#key-idea-delta-conservative-search)  
- [Adaptive Conservativeness Using Uncertainty](#adaptive-conservativeness-using-uncertainty)  
- [Experimental Setup and Benchmarks](#experimental-setup-and-benchmarks)  
- [Results](#results)  
- [Conclusion](#conclusion)

## Introduction and Motivation

In Biotechnology, designing biological sequences is a central problem.‚Ä®For example, we may want to design DNA sequences with high binding affinity, RNA molecules with specific folding properties, or proteins with enhanced fluorescence or stability. However, this is extremely challenging. The search space for the sequences grows exponentially Large with increasing sequence length, which makes brute-force search impossible.‚Ä®Furthermore, evaluating these sequences using an evaluating function, usually known as "Oracle function" or simply what we call "wet-lab experiments" is very expensive and takes a lot of time. Also, evaluation using Detailed simulation is not very cost and time effective as well.

To address this problem, we often use machine learning models called proxy models. But these proxies are imperfect, they are not always accurate and can make unreliable predictions, especially for unknown sequences.‚Ä®This paper addresses exactly this issue by proposing a conservative search strategy that improves robustness without sacrificing exploration. It introduces a novel off-policy search method called Delta-Conservative Search (Œ¥-CS), which is designed to enhance the robustness of generative policies, specifically Generative Flow Networks (GFlowNets), for designing biological sequences such as DNA, RNA, and proteins<a href="#ref-1" title="(2025) Improved Off-policy Reinforcement Learning in Biological Sequence Design">[1]</a>.

## Reinforcement Learning for Sequence Design

Biological sequence design can be framed as a sequential decision-making problem, we build a sequence one token at a time, and at the end we receive a score that reflects how good that sequence is according to a set objective or goal. This process makes reinforcement learning (RL) a perfect fit.

In this setting, an agent generates a sequence step by step. Each action corresponds to choosing the next token, for example; a nucleotide or amino acid. And once the full sequence is constructed, it is evaluated by an oracle (often an expensive wet-lab experiment or a high-fidelity simulator). Because querying the oracle is costly, modern approaches rely on proxy models that approximate the oracle and provide fast reward estimates during training itself.

### On-policy Reinforcement Learning

Early RL-based approaches for sequence design relied on on-policy methods, meaning the policy is trained only on data generated by its current version. A representative example is DyNA PPO, which applies Proximal Policy Optimization (PPO) within an active learning loop. On-policy methods have some appealing properties. They are relatively stable, simple, and align well with classical RL theory. However, they come with a major drawback which is poor data efficiency. Since they cannot effectively reuse previously collected sequences, large amounts of expensive oracle evaluations are needed to make progress. This drawback becomes highly problematic when dealing with Huge search spaces, where exploring is essential.

### Off-policy Reinforcement Learning

To address the limitations of On-policy RL, researchers have turned to off-policy RL methods, which can learn from data generated by older policies or even from fixed offline datasets. This is very effective in biology, where historical datasets and previously evaluated sequences are often available.

One effective off-policy framework for sequence design is Generative Flow Networks (GFlowNets). Instead of learning a single optimal sequence, GFlowNets learn a distribution over sequences, where the probability of sampling a sequence is proportional to its reward. This allows them to generate high-quality sequences , a crucial property when exploring biological landscapes. Another advantage of GFlowNets is their flexibility, they can mix offline data (existing evaluated sequences) with newly generated samples, leading to more stable and efficient training compared to on-policy RL methods.


## The Core Problem: Proxy Misspecification

Despite their promising performance, off-policy methods introduce a serious challenge, which is proxy model misspecification. Proxy models are trained on limited data and tend to make unreliable predictions on out-of-distribution (OOD) sequences. Off-policy methods, especially those that explore the search space widely, are more likely to generate such sequences.

This can lead to assigning greater reward to the sequence which may look promising but perform poor during evaluation by oracle function. The policy learns to exploit weaknesses in the proxy model rather than discovering meaningful biological sequences. Recent studies have shown that this issue becomes especially severe in large scale problems such as protein design, where the proxy model is particularly uncertain early in training. This creates a clear trade-off between exploration and reliability, which existing methods fail to balance effectively.

## Key Idea: Delta Conservative Search

The central idea of this paper is Delta-Conservative Search, abbreviated as Œ¥-CS. It is designed specifically to improve off-policy reinforcement learning methods like GFlowNets when proxy models are unreliable.‚Ä®The key intuition is simple, instead of exploring the entire sequence space blindly, we restrict exploration to regions where the proxy is more trustworthy. 

Œ¥-CS operates by starting from high-scoring sequences in the offline dataset and injecting noise in those sequences. Each token in a sequence is independently masked with probability Œ¥, where Œ¥ controls the degree of conservativeness. A smaller Œ¥ results in minor changes in the original sequence, while a larger Œ¥ allows more exploration as it changes the sequence to a larger extent. The masked sequences are then denoised sequentially using a GFlowNet policy, producing new candidate sequences that remain close to known high-quality sequences.

To further improve effectiveness of the search, Œ¥-CS adapts Œ¥ based on the uncertainty of the proxy model. When the proxy is uncertain, exploration is automatically restricted and when it is confident, broader exploration is allowed. This adaptive conservativeness enables Œ¥-CS to balance novelty and reliability, leading to more stable training and higher-quality sequence discovery across diverse biological design tasks.

### 1. Problem Formulation

We want to find sequences x ‚àà V<sup>L</sup>, Where V is the vocabulary (eg. amino acids or nucleotides) and L is the sequence length. We also look for some desired properties from these sequences such as binding affinity or enzymatic activity etc. These properties are evaluated by a black box oracle function ; f : V<sup>L</sup> ‚üπ R. Evaluation of this function (f) is very costly and time consuming because it involves wet-lab experiments and simulations. 

### 2. Active Learning for Biological Sequence Design

Biological sequence design often face problems due to limited experimental budgets, which makes it difficult to evaluate large numbers of candidate sequences using expensive laboratory testings. To address this, this paper adopts an active learning framework that iteratively improves both the predictive model and the generative policy using a small number of carefully selected queries. We will use some previous studeies for the Active Learning<a href="#ref-2" title="(2022) Biological sequence design with GFlowNets. In International Conference on Machine Learning (ICML)">[2]</a>.

![Figure 1: Active Learning for biological sequence design.]({{ site.baseurl }}/images/Fig1.png)
*Figure 1: Active Learning for biological sequence design.*


The active learning process has multiple rounds. In each round, a proxy model is trained on the currently available dataset (<i>Fig 1 : Step A</i>) to approximate the true oracle that evaluates sequence quality. This proxy enables rapid reward estimation and guides the search process. Next, a generative policy (<i>Fig 1 : Step B</i>), implemented using Generative Flow Networks (GFlowNets), is trained to propose promising new sequences based on the proxy‚Äôs predictions. Most important, this step intergates Œ¥-Conservative Search with GFlowNets, ensuring that proposed sequences remain within trustworthy regions of the search space.

Finally, a batch of generated sequences is evaluated using the true oracle, and the resulting data is added to the dataset for the next round (<i>Fig 1 : Step C</i>). By repeating this cycle, the framework efficiently balances exploration and exploitation, discovering higher-quality biological sequences while minimizing costly oracle evaluations.

### 3. Proxy Training

We train a proxy model fœï(x) using the offline dataset D<sub>t‚àí1</sub> at round t. We focus on minimising the mean squared error loss. We follow previos studies<a href="#ref-2" title="(2022) Biological sequence design with GFlowNets. In International Conference on Machine Learning (ICML)">[2]</a> to use the initial dataset D<sub>0</sub>.

$$
\mathcal{L}(\phi)=\mathbb{E}_{x \sim P_{D_{t-1}}(x)}\left[\left( f(x) - f_{\phi}(x) \right)^2\right]
$$

### 4. Policy Training with Œ¥-CS

The policy responsible for generating new biological sequences is trained using Generative Flow Networks (GFlowNets), an off-policy reinforcement learning framework designed to sample diverse high quality sequences. In this setting, the policy learns to generate sequences with probability proportional to a reward which is provided by proxy model.

$$
p(x;\theta) \propto R(x;\phi) = F(x;\phi)
$$

GFlowNets are generative models that learn a probability distribution over solutions rather than optimizing for a single best sequence. Each partial sequence is treated as a state, and actions correspond to adding tokens.‚Ä®Once a complete sequence is formed, it receives a reward. But, a major challenge in policy training is that proxy models can be highly unreliable for out-of-distribution (OOD) sequences, which can destabilize learning. To solve this problem, Œ¥-Conservative Search (Œ¥-CS) is integrated directly into policy training. Instead of training only on fully generated sequences, the policy is trained on trajectories obtained by first disturbing high-scoring offline sequences and then reconstructing them.

### 5. Policy and Learning Objective.

The policy consists of two components, a forward policy (P<sub>F</sub>) and a backward policy (P<sub>B</sub>). The forward Policy generates sequences token by token. It starts with an empty sequence (s<sub>0</sub>). After L steps we get the full sequence (s<sub>L</sub> = x), where L is the sequence length. The backward policy is used for modelling the probability of backtracking from the terminal state. The backward policy is deterministic, which simplifies training and makes GFlowNets equivalent to certain soft off-policy RL methods.

$$
P_F(\tau; \theta) = \prod_{i=1}^{L} P_F(s_i \mid s_{i-1}; \theta)
$$

The policy is trained to minimise the Trajectory balance (TB) Loss, which enforces consistency between forward and backward probabilities and rewards. However, the main challenge is that proxy models can produce highly unreliable rewards for out-of-distribution sequences.‚Ä®Training on such trajectories can destabilize learning and degrade performance. The Œ¥-CS comes into play here.

$$
\mathcal{L}_{\mathrm{TB}}(\tau; \theta) = \left(\log \frac{Z_\theta \, P_F(\tau; \theta)}{R(x; \phi)}\right)^2
$$

### 6. Œ¥-Conservative Search (Œ¥-CS)

Œ¥-CS is an off policy search method which modifies off-policy trajectory generation and enables controllable exploaration in the search space through a control parameter Œ¥. The Œ¥ defines the probability of masking the tokens in the noise injection step. The Algorithm proceeds as follows :-

#### 6.1 Sampling high scoring offline sequences

To start from reliable points, sequences are sampled from a rank-based reweighted prior. Ee sample a reference sequence x from the prior distribution P<sub>D<sub>t-1</sub></sub>. Higher-ranked sequences are always assigned greater weights. This biases exploration toward promising regions while still allowing diversity. We use rank-based prioritization <a href="#ref-3" title="(2020) Sample-efficient optimization in the latent space of deep generative models via weighted retraining. In Advances in Neural Information Processing Systems (NeurIPS)">[3]</a>

$$
w(x; \mathcal{D}_{t-1}, k) \propto \frac{1}{kN + \mathrm{rank}_{f,\mathcal{D}_{t-1}}(x)}
$$

#### 6.2 Injecting Noise

In this step we introduce some noise into the obtained sequence from previosu step. In this step we use a noise injection policy which masks each individual token with a probability Œ¥. This ultimately determines how much of the original sequence is preserved. Smaller Œ¥ values result in conservation , while larger values encourage more exploration (Œ¥ ‚àà [0,1]). 

$$
P_{\text{noise}}(\tilde{x} \mid x, \delta) = \prod_{i=1}^{L} \left[\delta \cdot \mathbb{I} \{\tilde{e}_i = \text{[MASK]} \}+(1 - \delta) \cdot \mathbb{I}
\{\tilde{e}_i= e_i \}\right]
$$

#### 6.3 Denoising

In this step, the masked sequences from the previous step is reconstructed sequentially using the GFlowNet forward policy by predicting tokens from left to right.
The probability of denoising next token ƒï<sub>t</sub> from previously denoised subsquence ≈ù<sub>t‚àí1</sub> is :

$$
P_{\text{denoise}}(\hat{x} \mid \tilde{x}; \theta) =
\prod_{t=1}^{L}
\begin{cases}
\mathbb{I}\{\hat{e}_t = \tilde{e}_t\}, & \tilde{e}_t \neq \text{[MASK]}, \\[6pt]
P_F(\hat{s}_t \mid \hat{s}_{t-1}; \theta), & \tilde{e}_t = \text{[MASK]}.
\end{cases}
$$


After that, to obtain a fully reconstructed sequence $\hat{x}$ = ≈ù<sub>L</sub> we sample from denoising policy. The GFlowNet policy reconstructs masked tokens one by one from left to right, producing new sequences $\hat{x}$ whose level of novelty is controlled by the conservativeness parameter Œ¥.

$$
P_{\text{denoise}}(\hat{x} \mid \tilde{x}; \theta) = \prod_{t=1}^{L} P_{\text{denoise}}(\hat{e}_t \mid \hat{s}_{t-1}, \tilde{x}; \theta).
$$

## Adaptive Conservativeness Using Uncertainty

Choosing an appropriate level of conservativeness is very important for effective exploration in biological sequence design. A fixed value of the conservativeness parameter Œ¥ may be not optimal, as proxy models can be confident in some regions of the sequence space while highly uncertain in others. To address this, the proposed method introduces adaptive conservativeness, where the extent of exploration is dynamically adjusted based on the proxy model‚Äôs uncertainty. The conservativeness parameter is defined as a function of each sequence.

$$
\delta(x;\sigma) = \delta_{\mathrm{const.}} - \lambda \sigma(x)
$$

where œÉ(x) denotes the uncertainty of the proxy model's prediction for sequence x, Œ¥<sub>const</sub> is a base conservativeness level, and Œª controls the strength of the adjustment. The resulting value is clamped to remain within [0,1]. This encourages more conservative search when the proxy is uncertain, restricting exploration to nearby, reliable regions, while allowing broader exploration when the proxy is confident. By aligning the search behaviour with model confidence, adaptive conservativeness improves effectiveness against proxy misspecification and leads to more stable and effective policy training.


## Experimental Setup and Benchmarks


## Results


## Conclusion



<a id="ref-1"></a>
<a id="ref-2"></a>
<a id="ref-3"></a>
<a id="ref-4"></a>
<a id="ref-5"></a>
<a id="ref-6"></a>
<a id="ref-7"></a>
<a id="ref-8"></a>
## References

1. Hyeonah Kim, Minsu Kim, Taeyoung Yun, Sanghyeok Choi, Emmanuel Bengio, Alex Hern√°ndez-Garc√≠a, Jinkyoo Park (2025). Improved Off-policy Reinforcement Learning in Biological Sequence Design. [https://arxiv.org/abs/2102.09844  ](https://arxiv.org/abs/2410.04461)
2. Jain, M., Bengio, E., Hernandez-Garcia, A., Rector-Brooks, J., Dossou, B. F., Ekbote, C. A., Fu, J., Zhang, T., Kilgour, M., Zhang, D., et al. (2022) Biological sequence design with GFlowNets. In International Conference on Machine Learning (ICML).  
3. Tripp, A., Daxberger, E., and Hern¬¥ andez-Lobato, J. M. (2020) Sample-efficient optimization in the latent space of deep generative models via weighted retraining. In Advances in Neural Information Processing Systems (NeurIPS).  
4. Lu, C., Wu, H., Shen, R., & Cao, Y. (2021). Pocket2Mol: Efficient Molecular Generation Based on Binding Pockets. *NeurIPS 2021*. https://arxiv.org/abs/2110.07875  
5. Ganea, O.-E., Huang, J., Bunne, C., & Krause, A. (2021). Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking. *arXiv preprint arXiv:2111.07786*. https://arxiv.org/abs/2111.07786  
6. Zhavoronkov, A., Ivanenkov, Y. A., Aliper, A., et al. (2019). Deep learning enables rapid identification of potent DDR1 kinase inhibitors. *Nature Biotechnology*, 37(9), 1038‚Äì1040. https://doi.org/10.1038/s41587-019-0224-x  
7. Berman, H. M., et al. (2000). The Protein Data Bank. *Nucleic Acids Research*, 28(1), 235‚Äì242. https://doi.org/10.1093/nar/28.1.235  
8. Huuskonen, J. (2000). Estimation of Aqueous Solubility for a Diverse Set of Organic Compounds Based on Molecular Topology. *Journal of Chemical Information and Computer Sciences*, 40(3), 773‚Äì777. https://doi.org/10.1021/ci990307l  


