---
title: "Improved Off-policy Reinforcement Learning in Biological Sequence Design"
date: 2026-02-16
permalink: /posts/2026/02/RL_seq/
tags:
  - deep-learning
  - Reinforcement-Learning
  - Biological-Sequence-Design
  - Proxy-training
  - ùúπ-Conservative-search
  - GFlowNets
math: true
---

## Abstract

Designing biological sequences such as DNA, RNA, and proteins is a central challenge in biotechnology due to the Huge search space and the high cost of estimation. Recent reinforcement learning (RL) approaches rely on proxy models to estimate quality, but these models often fail when exploring regions with unknown data. This paper introduces Œ¥-Conservative Search (Œ¥-CS), a novel off-policy reinforcement learning strategy that balances exploration and reliability in biological sequence design. The method focus on high-scoring known sequences by randomly masking tokens and then denoising them using a generative policy, ensuring that new candidates remain close to reliable regions of the search space. The degree of conservativeness is adaptively adjusted based on the proxy model‚Äôs uncertainty. Experiments across DNA, RNA, and protein design tasks demonstrate that Œ¥-CS consistently discovers higher quality sequences than existing methods. This makes Œ¥-CS a scalable and reliable framework for real-world biological design.

## Table of Contents

- [Introduction and Motivation](#introduction-and-motivation)  
- [Reinforcement Learning for Sequence Design](#reinforcement-learning-for-Sequence-design)
- [The Core Problem: Proxy Misspecification](#the-core-problem-proxy-misspecification)  
- [Key Idea: Delta Conservative Search](#key-idea-delta-conservative-search)  
- [Adaptive Conservativeness Using Uncertainty](#adaptive-conservativeness-using-uncertainty)  
- [Experimental Setup and Benchmarks](#experimental-setup-and-benchmarks)  
- [Results](#results)  
- [Conclusion](#conclusion)

## Introduction and Motivation

In Biotechnology, designing biological sequences is a central problem.‚Ä®For example, we may want to design DNA sequences with high binding affinity, RNA molecules with specific folding properties, or proteins with enhanced fluorescence or stability. However, this is extremely challenging. The search space for the sequences grows exponentially Large with increasing sequence length, which makes brute-force search impossible.‚Ä®Furthermore, evaluating these sequences using an evaluating function, usually known as "Oracle function" or simply what we call "wet-lab experiments" is very expensive and takes a lot of time. Also, evaluation using Detailed simulation is not very cost and time effective as well.

To address this problem, we often use machine learning models called proxy models. But these proxies are imperfect, they are not always accurate and can make unreliable predictions, especially for unknown sequences.‚Ä®This paper addresses exactly this issue by proposing a conservative search strategy that improves robustness without sacrificing exploration. It introduces a novel off-policy search method called Delta-Conservative Search (Œ¥-CS), which is designed to enhance the robustness of generative policies, specifically Generative Flow Networks (GFlowNets), for designing biological sequences such as DNA, RNA, and proteins<a href="#ref-1" title="(2025) Improved Off-policy Reinforcement Learning in Biological Sequence Design">[1]</a>.

## Reinforcement Learning for Sequence Design

Biological sequence design can be framed as a sequential decision-making problem, we build a sequence one token at a time, and at the end we receive a score that reflects how good that sequence is according to a set objective or goal. This process makes reinforcement learning (RL) a perfect fit.

In this setting, an agent generates a sequence step by step. Each action corresponds to choosing the next token, for example; a nucleotide or amino acid. And once the full sequence is constructed, it is evaluated by an oracle (often an expensive wet-lab experiment or a high-fidelity simulator). Because querying the oracle is costly, modern approaches rely on proxy models that approximate the oracle and provide fast reward estimates during training itself.

### On-policy Reinforcement Learning

Early RL-based approaches for sequence design relied on on-policy methods, meaning the policy is trained only on data generated by its current version. A representative example is DyNA PPO, which applies Proximal Policy Optimization (PPO) within an active learning loop. On-policy methods have some appealing properties. They are relatively stable, simple, and align well with classical RL theory. However, they come with a major drawback which is poor data efficiency. Since they cannot effectively reuse previously collected sequences, large amounts of expensive oracle evaluations are needed to make progress. This drawback becomes highly problematic when dealing with Huge search spaces, where exploring is essential.

### Off-policy Reinforcement Learning

To address the limitations of On-policy RL, researchers have turned to off-policy RL methods, which can learn from data generated by older policies or even from fixed offline datasets. This is very effective in biology, where historical datasets and previously evaluated sequences are often available.

One effective off-policy framework for sequence design is Generative Flow Networks (GFlowNets). Instead of learning a single optimal sequence, GFlowNets learn a distribution over sequences, where the probability of sampling a sequence is proportional to its reward. This allows them to generate high-quality sequences , a crucial property when exploring biological landscapes. Another advantage of GFlowNets is their flexibility, they can mix offline data (existing evaluated sequences) with newly generated samples, leading to more stable and efficient training compared to on-policy RL methods.


## The Core Problem: Proxy Misspecification

Despite their promising performance, off-policy methods introduce a serious challenge, which is proxy model misspecification. Proxy models are trained on limited data and tend to make unreliable predictions on out-of-distribution (OOD) sequences. Off-policy methods, especially those that explore the search space widely, are more likely to generate such sequences.

This can lead to assigning greater reward to the sequence which may look promising but perform poor during evaluation by oracle function. The policy learns to exploit weaknesses in the proxy model rather than discovering meaningful biological sequences. Recent studies have shown that this issue becomes especially severe in large scale problems such as protein design, where the proxy model is particularly uncertain early in training. This creates a clear trade-off between exploration and reliability, which existing methods fail to balance effectively.

## Key Idea: Delta Conservative Search

The central idea of this paper is Delta-Conservative Search, abbreviated as Œ¥-CS. It is designed specifically to improve off-policy reinforcement learning methods like GFlowNets when proxy models are unreliable.‚Ä®The key intuition is simple, instead of exploring the entire sequence space blindly, we restrict exploration to regions where the proxy is more trustworthy. 

Œ¥-CS operates by starting from high-scoring sequences in the offline dataset and injecting noise in those sequences. Each token in a sequence is independently masked with probability Œ¥, where Œ¥ controls the degree of conservativeness. A smaller Œ¥ results in minor changes in the original sequence, while a larger Œ¥ allows more exploration as it changes the sequence to a larger extent. The masked sequences are then denoised sequentially using a GFlowNet policy, producing new candidate sequences that remain close to known high-quality sequences.

To further improve effectiveness of the search, Œ¥-CS adapts Œ¥ based on the uncertainty of the proxy model. When the proxy is uncertain, exploration is automatically restricted and when it is confident, broader exploration is allowed. This adaptive conservativeness enables Œ¥-CS to balance novelty and reliability, leading to more stable training and higher-quality sequence discovery across diverse biological design tasks.

### Problem Formulation

We want to find sequences x ‚àà V^L, Where V is the vocabulary (eg. amino acids or nucleotides) and L is the sequence length. We also look for some desired properties from these sequences such as binding affinity or enzymatic activity etc. These properties are evaluated by a black box oracle function ; f : V^L -> R. Evaluation of this function (f) is very costly and time consuming because it involves wet-lab experiments and simulations. 



## Adaptive Conservativeness Using Uncertainty


## Experimental Setup and Benchmarks


## Results


## Conclusion



<a id="ref-1"></a>
<a id="ref-2"></a>
<a id="ref-3"></a>
<a id="ref-4"></a>
<a id="ref-5"></a>
<a id="ref-6"></a>
<a id="ref-7"></a>
<a id="ref-8"></a>
## References

1. Hyeonah Kim, Minsu Kim, Taeyoung Yun, Sanghyeok Choi, Emmanuel Bengio, Alex Hern√°ndez-Garc√≠a, Jinkyoo Park (2025). Improved Off-policy Reinforcement Learning in Biological Sequence Design. [https://arxiv.org/abs/2102.09844  ](https://arxiv.org/abs/2410.04461)
2. Satorras, V. G., Hoogeboom, E., Fuchs, F. B., & Welling, M. (2021). E(n) Equivariant Graph Neural Networks. *arXiv preprint arXiv:2102.09844*. https://arxiv.org/abs/2102.09844  
3. Xu, M., Wang, Y., Hu, W., & Leskovec, J. (2021). GeoDiff: A Geometric Diffusion Model for Molecular Conformation Generation. *ICLR 2022*. https://arxiv.org/abs/2203.02923  
4. Lu, C., Wu, H., Shen, R., & Cao, Y. (2021). Pocket2Mol: Efficient Molecular Generation Based on Binding Pockets. *NeurIPS 2021*. https://arxiv.org/abs/2110.07875  
5. Ganea, O.-E., Huang, J., Bunne, C., & Krause, A. (2021). Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking. *arXiv preprint arXiv:2111.07786*. https://arxiv.org/abs/2111.07786  
6. Zhavoronkov, A., Ivanenkov, Y. A., Aliper, A., et al. (2019). Deep learning enables rapid identification of potent DDR1 kinase inhibitors. *Nature Biotechnology*, 37(9), 1038‚Äì1040. https://doi.org/10.1038/s41587-019-0224-x  
7. Berman, H. M., et al. (2000). The Protein Data Bank. *Nucleic Acids Research*, 28(1), 235‚Äì242. https://doi.org/10.1093/nar/28.1.235  
8. Huuskonen, J. (2000). Estimation of Aqueous Solubility for a Diverse Set of Organic Compounds Based on Molecular Topology. *Journal of Chemical Information and Computer Sciences*, 40(3), 773‚Äì777. https://doi.org/10.1021/ci990307l  


